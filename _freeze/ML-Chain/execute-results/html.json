{
  "hash": "03eb7b5b36366cb7e8f5f1bd526ff7c3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"ML-Chain\"\neditor: visual\n---\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(terra)      \n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nterra 1.8.42\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(sf)          \n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLinking to GEOS 3.13.0, GDAL 3.8.5, PROJ 9.5.1; sf_use_s2() is TRUE\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(tidyverse)   \n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::extract() masks terra::extract()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(tidymodels)  \n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.8     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.8     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.3.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ dials::buffer()   masks terra::buffer()\n✖ scales::discard() masks purrr::discard()\n✖ tidyr::extract()  masks terra::extract()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n✖ recipes::update() masks terra::update(), stats::update()\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(remotes)\n#for getting area of interest shapefiles (Mike's package from github)\nlibrary(AOI)\n# for gridded data access (Mike's package from github )\nlibrary(climateR)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'climateR'\n\nThe following object is masked from 'package:readr':\n\n    parse_date\n\nThe following object is masked from 'package:graphics':\n\n    plot\n\nThe following object is masked from 'package:base':\n\n    plot\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# Set the coordinate reference system (CRS) from an existing raster WHY NOT DO THIS WITH ALL \ncrs = crs(rast(\"data/sl_forb_current.tiff\"))\n\n# Get the state boundary for Colorado and project it to the same CRS\nco = AOI::aoi_get(state = \"CO\") |> \n  sf::st_transform(crs)\n\n# Load two raster layers (forb and grass) and crop them to Colorado\nr = terra::rast(c('data/sl_forb_current.tiff',\n                  'data/sl_grass_current.tiff')) |> \n  terra::crop(co)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# Load mid-century projections (future climate scenario) and crop them\nr2 = terra::rast(c('data/sl_forb_mid_585.tiff',\n                   'data/sl_grass_mid_585.tiff')) |> \n  terra::crop(co)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# Extract all current raster values\nv = values(r)\n\n# Identify non-NA rows\nidx <- which(!apply(is.na(v), 1, any))\n\n# Remove rows with NA values\nv   <- na.omit(v)\n\n# Scale the data (mean = 0, sd = 1)\nvs  <- scale(v)\n\n# this stransforms the data so that in every column mean=0 stddev= 1. This is important for k-menas because it creates equal weighting of varibles on diff scales, and fair distance computation; k-means use euclidean distance, which is sensitive to scale; scalinf ensures all var contribute equally to the clustering. This all avoids bias towards one var or another. \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# Perform k-means clustering (5 clusters, max 100 iterations)\n#k-mean= clustering algorithm used in unpupervised machine learning () to group data points into specified number of clusters, based on similarity.\n\n#smaller k's= simplier intepration, less overfitting, faster computation; but you may miss important sub-group differences \n#larger k's= can revealsubtle patters or subtypes, and better to fit to data; meaning it may reduce within-cluster variance \n# There are several ways to chose the best k-mean, I found a way called the elbow method : \n# Elbow Method: Compute WCSS for k = 1 to 10\n\n#wcss <- numeric(10)\n\n#for (k in 1:10) {\n  #set.seed(42)  # For reproducibility\n  #km <- kmeans(vs, centers = k, iter.max = 100)\n  #wcss[k] <- km$tot.withinss\n#}\n\n# Plot the elbow curve\n#plot(1:10, wcss, type = \"b\", pch = 19,\n    # xlab = \"Number of Clusters (k)\",\n     #ylab = \"Total Within-Cluster Sum of Squares (WCSS)\",\n     #main = \"Elbow Method for Determining Optimal k\")\n\n#this shows you the \"elbow\" point, where increasing the k has diminishing returns\n```\n:::\n\n\n\n\n\nMaybe include (ask Mike)\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# 5 is defined by you - no right or wrong answer\n# This take a while to run so be sure to run, save, and comment in your code\nE <- kmeans(vs, 5, iter.max = 100)\n\n# Create a new raster for cluster results\nclus_raster <- r[[1]]\nvalues(clus_raster) <- NA\nclus_raster[idx] <- E$cluster\n#Purpose: turn the clustering results into spatial raster that matches the original data\n\n# Plot the cluster map\nplot(clus_raster, col = RColorBrewer::brewer.pal(5, \"Spectral\"))\n```\n\n::: {.cell-output-display}\n![](ML-Chain_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code .hidden}\n#visually inspect hwo the clusters are distributes; helps intepret how regions are simial by climate/ invasion projections \n\n# Combine current, future, and cluster rasters into a single stack\nrs = c(r, r2, cl = clus_raster) |> \n  setNames(c(names(r), names(r2), \"cluster\"))\n#this builds a multi-layer raster stack for future analysis. This step is key for linking cluster ID's to present and future landscape characteristics \n\n\n# Define climate variables to retrieve\n# We are using the following variables:\nparams = c('tmax', \"soil\", \"srad\", \"ppt\", \"vpd\", \"pet\")\n\n#tmax= max air temp: critical for determine phsyiological threasholf and growing season length\n#ppt= determines soil moisture and plant productivity. \n#soil- soil mositure content; influences plant establishment and competition. Some invasive species can tolerate drought better (or explot mosit soil) better than some natives \n#vpd=reflects atmospheric dryness; high vpd stresses plants, which can reduce native resistance and create openings for invasives\n#pet=potential evapostranspiration; helps asses water stress conditions, which invasices may exploit\n#srad= solar radiation; influences photsynthesis and growth which can cooralte with biomass and influence comp. dynamics \n```\n:::\n\n\n\n\n\nThis map shows:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# Download TerraClim climate data for Colorado (2015–2020 average)\ngM = climateR::getTerraClim(co, params, \n                            startDate = \"2015-01-01\", \n                            endDate   = \"2020-12-31\") \n\n\n# Load field plot data (data from 2007) and join with plot geometries\npts <- inner_join(read_csv(\"data/CO_INVASIVE_SUBPLOT_SPP.csv\"), \n                  read_csv(\"data/CO_PLOTGEOM.csv\"), \n                  by = c(\"PLT_CN\" = 'CN')) %>% \n  st_as_sf(coords = c(\"LON\", 'LAT'), crs = 4326) |> \n  st_transform(crs)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nRows: 3975 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (2): VEG_FLDSPCD, VEG_SPCD\ndbl  (13): CN, PLT_CN, INVYR, STATECD, UNITCD, COUNTYCD, PLOT, SUBP, CONDID,...\nlgl   (1): MODIFIED_DATE\ndttm  (1): CREATED_DATE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 29168 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (3): ECOSUBCD, ROADLESSCD, FVS_VARIANT\ndbl  (16): CN, STATECD, INVYR, UNITCD, COUNTYCD, PLOT, LAT, LON, CONGCD, HUC...\nlgl   (3): FVS_DISTRICT, ECO_UNIT_PNW, PRECIPITATION\ndttm  (2): CREATED_DATE, MODIFIED_DATE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# Extract raster and climate data to points\nxx = bind_cols(\n  terra::extract(rs, pts),  \n  terra::extract(rast(map(gM, mean)), pts)[,-1]\n) |> \n  drop_na()\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nWarning: [extract] transforming vector data to the CRS of the raster\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nwrite_csv(xx, \"data/xx.csv\")\n#saves the cleaned, extracted data to a CSV file \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# -------------------\n# Start the modeling workflow\n# -------------------\n\n# Split data into training and testing sets, stratified by cluster\nxx = read_csv(\"data/xx.csv\") \n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nRows: 3965 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (12): ID, sl_forb_current, sl_grass_current, sl_forb_mid_585, sl_grass_m...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nxx = replace(xx, xx == 0 , 1)\n#replace 0's with 1's; do this to avoid transformation/comp issues. In an eco sense, you do this to avoid flasley interpreting 0 as absence if the data is just very low. It also prevents modeling distorions \n\n#set seed and do inital split \nset.seed(123)\ns = initial_split(xx, strata = cluster)\ntraining = training(s)\ntesting = testing(s)\n\n# Create 10-fold cross-validation splits\nfolds = vfold_cv(training, v = 10)\n\n# Define various models\nlm_mod = linear_reg() |> \n  set_engine(\"lm\") |> \n  set_mode(\"regression\")\n\nb_mod = boost_tree() |> \n  set_engine(\"xgboost\") |> \n  set_mode(\"regression\")\n\nb_mod2 = boost_tree() |> \n  set_engine(\"lightgbm\") |> \n  set_mode(\"regression\")\n\nrf_mod = rand_forest() |> \n  set_engine(\"ranger\", importance = \"impurity\") |> \n  set_mode(\"regression\")\n\nnn_mod = mlp() |> \n  set_engine(\"nnet\") |> \n  set_mode(\"regression\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(bonsai)  # Needed for using lightgbm in tidymodels\n\n# Build a recipe to preprocess the data (drop unwanted columns)\nrec = recipe(sl_forb_mid_585 ~ ., data = training) |> \n  step_rm(sl_grass_mid_585, ID) |> \n  step_normalize(all_predictors())\n\n# Create a workflow set: recipe + all models\nwf = workflow_set(list(rec),\n                  list(lm_mod, \n                       xgboost = b_mod, \n                       lightgbm = b_mod2, \n                       nn_mod, \n                       rf_mod))  |> \n  workflow_map(resamples = folds)\n\n# Visualize workflow performance\nautoplot(wf)\n```\n\n::: {.cell-output-display}\n![](ML-Chain_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code .hidden}\n# Collect model evaluation metrics\ncollect_metrics(wf) |> \n  filter(.metric == \"rsq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 9\n  wflow_id          .config preproc model .metric .estimator  mean     n std_err\n  <chr>             <chr>   <chr>   <chr> <chr>   <chr>      <dbl> <int>   <dbl>\n1 recipe_linear_reg Prepro… recipe  line… rsq     standard   0.562    10 0.0159 \n2 recipe_xgboost    Prepro… recipe  boos… rsq     standard   0.897    10 0.00999\n3 recipe_lightgbm   Prepro… recipe  boos… rsq     standard   0.928    10 0.00752\n4 recipe_mlp        Prepro… recipe  mlp   rsq     standard   0.667    10 0.0200 \n5 recipe_rand_fore… Prepro… recipe  rand… rsq     standard   0.944    10 0.00873\n```\n\n\n:::\n:::\n\n\n\n\n\nwant to move forward with random forest model because it has the lowest root mean sqaured error and highest r-sqaured value.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# Focus on random forest (rf_mod) for tuning\n\n\n# Build a single workflow with random forest model\nwf = workflow() |> \n  add_recipe(rec) |> \n  add_model(rf_mod) \n\n# Fit the random forest to training data\nwf_f = wf |> \n  fit(data = training) \n\n# Visualize feature importance (tells you which feature/variable was most important in making predictions)\nvip::vip(wf_f)\n```\n\n::: {.cell-output-display}\n![](ML-Chain_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code .hidden}\n# Tune hyperparameters of random forest\nrf_t = rand_forest(trees = tune(), \n                   mtry = tune(), \n                   min_n = tune()) |> \n  set_engine(\"ranger\") |> \n  set_mode(\"regression\")\n#trees= the number of decison trees; more means better preformance (to a point) but slower traning. \n#mtry= controls the number of predictor varibales randomly selected at each split. More= less randomness\n#min_n= controls the number of data points a node must have to be split further; high values= underfitting, low values= overfitting \n\n# Perform grid search: find the best combo of hyperparameters \n# This takes a while!\ntg = tune_grid(rf_t, \n               preprocessor = rec,\n               resamples = folds, \n               grid = 25)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\ni Creating pre-processing data to finalize unknown parameter: mtry\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# Show best performing hyperparameter set based on metric of choice. I chose R2\nshow_best(tg, metric = \"rsq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     6  1167     2 rsq     standard   0.948    10 0.00907 Preprocessor1_Model17\n2     3   667     3 rsq     standard   0.946    10 0.00872 Preprocessor1_Model08\n3     7   417     6 rsq     standard   0.946    10 0.00918 Preprocessor1_Model20\n4     5  1833     8 rsq     standard   0.942    10 0.00920 Preprocessor1_Model13\n5     8  1666     9 rsq     standard   0.942    10 0.00941 Preprocessor1_Model22\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# Finalize workflow with best hyperparameters\nworkflow <- finalize_workflow(wf,  \n                              select_best(tg, metric = \"rsq\"))\n\n# Final fit on test set\n(final_fit <- last_fit(workflow, s))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits             id               .metrics .notes   .predictions .workflow \n  <list>             <chr>            <list>   <list>   <list>       <list>    \n1 <split [2973/992]> train/test split <tibble> <tibble> <tibble>     <workflow>\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# Collect evaluation metrics on test set\ncollect_metrics(final_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.253 Preprocessor1_Model1\n2 rsq     standard       0.965 Preprocessor1_Model1\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# Plot predicted vs observed values (test set)\ncollect_predictions(final_fit) |> \n  ggplot(aes(x = .pred, y = sl_forb_mid_585 )) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  geom_abline(slope = 1, intercept = 0) +\n  theme_bw() +\n  labs(x = \"Predicted\", y = \"Observed\") +\n  coord_equal()\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](ML-Chain_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n\n```{.r .cell-code .hidden}\n# -------------------\n# Predict on all data (for visualization)\n# -------------------\n\n# Fit model to all data\nmod = fit(workflow, xx)\n\n# Visualize predictions across clusters\naugment(mod, new_data = xx) |> \n  ggplot(aes(x = .pred, y = sl_forb_mid_585, color = as.factor(cluster))) +\n  scale_color_brewer(palette = \"Spectral\") +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0) +\n  theme_bw() +\n  labs(x = \"Predicted\", y = \"Observed\") +\n  coord_equal()\n```\n\n::: {.cell-output-display}\n![](ML-Chain_files/figure-html/unnamed-chunk-7-3.png){width=672}\n:::\n\n```{.r .cell-code .hidden}\n# Re-plot the cluster raster\nplot(clus_raster, col = RColorBrewer::brewer.pal(5, \"Spectral\"))\n```\n\n::: {.cell-output-display}\n![](ML-Chain_files/figure-html/unnamed-chunk-7-4.png){width=672}\n:::\n:::\n",
    "supporting": [
      "ML-Chain_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}